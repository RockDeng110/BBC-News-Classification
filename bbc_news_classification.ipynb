{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "04caeb64",
      "metadata": {
        "id": "04caeb64"
      },
      "source": [
        "# **BBC News Classification**\n",
        "\n",
        "### **1\\. Introduction**\n",
        "\n",
        "#### **1.1 Objective statement**\n",
        "Classify the dataset of BBC news articles into five categories (business, entertainment, politics, sport, tech).  \n",
        "#### **1.2 Plan**:\n",
        "  * Implement Unsupervised learning on it\n",
        "  * Then compare it with supervised learning.\n",
        "\n",
        "\n",
        "### **1.3 Data description**\n",
        "\n",
        "**File descriptions**\n",
        "\n",
        "|File name| Descriptions|\n",
        "|--|--|\n",
        "|Train.csv|The BBC News Train dataset which has 1490 records|\n",
        "|Test.csv|The BBC News test dataset which has 736 records|\n",
        "|Solution.csv |The BBC News Sample， a sample submission file in the correct format|\n",
        "\n",
        "**Data fields**\n",
        "\n",
        "|Field name| Descriptions|\n",
        "|--|--|\n",
        "|ArticleId|Article id unique # given to the record|\n",
        "|Article|text of the header and article|\n",
        "|Category |cateogry of the article (tech, business, sport, entertainment, politics）|\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2\\. Exploratory Data Analysis (EDA)**\n",
        "\n",
        "* **2.1. Load the Data:**  \n",
        "  * Load the training dataset using pandas.  \n",
        "  * Display the first few rows (.head()) of the dataframe.  \n",
        "  * Use .info() and .describe() to get a summary of the data.  \n"
      ],
      "metadata": {
        "id": "wiIwcKAeNq9j"
      },
      "id": "wiIwcKAeNq9j"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "raw_url_train = 'https://raw.githubusercontent.com/RockDeng110/BBC-News-Classification/main/datasets/BBC%20News%20Train.csv'\n",
        "raw_url_test = 'https://raw.githubusercontent.com/RockDeng110/BBC-News-Classification/main/datasets/BBC%20News%20Test.csv'\n",
        "raw_url_sample = 'https://raw.githubusercontent.com/RockDeng110/BBC-News-Classification/main/datasets/BBC%20News%20Sample%20Solution.csv'\n",
        "\n",
        "\n",
        "df_train = pd.read_csv(raw_url_train)\n",
        "df_test = pd.read_csv(raw_url_test)\n",
        "df_sample = pd.read_csv(raw_url_sample)\n",
        "\n",
        "\n",
        "def get_df_summary(df, df_name):\n",
        "  # print out name of df\n",
        "  print(f'===== Summary of  {df_name}:')\n",
        "  print(f'[DataFrame head]:')\n",
        "  print(df.head())\n",
        "  print(f'[DataFrame info]:')\n",
        "  df.info()\n",
        "  print(f'[DataFrame describe]:')\n",
        "  print(df.describe())\n",
        "\n",
        "get_df_summary(df_train, \"df_train\")\n",
        "get_df_summary(df_test, \"df_test\")\n",
        "get_df_summary(df_sample, \"df_sample\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTPhaqdNPZqz",
        "outputId": "c469fb62-a054-468f-99fd-6a66df038659"
      },
      "id": "wTPhaqdNPZqz",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===== Summary of  df_train:\n",
            "[DataFrame head]:\n",
            "   ArticleId                                               Text  Category\n",
            "0       1833  worldcom ex-boss launches defence lawyers defe...  business\n",
            "1        154  german business confidence slides german busin...  business\n",
            "2       1101  bbc poll indicates economic gloom citizens in ...  business\n",
            "3       1976  lifestyle  governs mobile choice  faster  bett...      tech\n",
            "4        917  enron bosses in $168m payout eighteen former e...  business\n",
            "[DataFrame info]:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1490 entries, 0 to 1489\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   ArticleId  1490 non-null   int64 \n",
            " 1   Text       1490 non-null   object\n",
            " 2   Category   1490 non-null   object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 35.1+ KB\n",
            "[DataFrame describe]:\n",
            "         ArticleId\n",
            "count  1490.000000\n",
            "mean   1119.696644\n",
            "std     641.826283\n",
            "min       2.000000\n",
            "25%     565.250000\n",
            "50%    1112.500000\n",
            "75%    1680.750000\n",
            "max    2224.000000\n",
            "===== Summary of  df_test:\n",
            "[DataFrame head]:\n",
            "   ArticleId                                               Text\n",
            "0       1018  qpr keeper day heads for preston queens park r...\n",
            "1       1319  software watching while you work software that...\n",
            "2       1138  d arcy injury adds to ireland woe gordon d arc...\n",
            "3        459  india s reliance family feud heats up the ongo...\n",
            "4       1020  boro suffer morrison injury blow middlesbrough...\n",
            "[DataFrame info]:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 735 entries, 0 to 734\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   ArticleId  735 non-null    int64 \n",
            " 1   Text       735 non-null    object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 11.6+ KB\n",
            "[DataFrame describe]:\n",
            "         ArticleId\n",
            "count   735.000000\n",
            "mean   1099.424490\n",
            "std     643.925514\n",
            "min       1.000000\n",
            "25%     545.500000\n",
            "50%    1116.000000\n",
            "75%    1657.500000\n",
            "max    2225.000000\n",
            "===== Summary of  df_sample:\n",
            "[DataFrame head]:\n",
            "   ArticleId       Category\n",
            "0       1018          sport\n",
            "1       1319           tech\n",
            "2       1138       business\n",
            "3        459  entertainment\n",
            "4       1020       politics\n",
            "[DataFrame info]:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 735 entries, 0 to 734\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   ArticleId  735 non-null    int64 \n",
            " 1   Category   735 non-null    object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 11.6+ KB\n",
            "[DataFrame describe]:\n",
            "         ArticleId\n",
            "count   735.000000\n",
            "mean   1099.424490\n",
            "std     643.925514\n",
            "min       1.000000\n",
            "25%     545.500000\n",
            "50%    1116.000000\n",
            "75%    1657.500000\n",
            "max    2225.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **2.2. Data Cleaning:**  \n",
        "  * Check for missing values (.isnull().sum()) and decide on a strategy to handle them if any exist.  \n",
        "  * Check for duplicate articles and remove them.  \n",
        "* **2.3. Data Visualization:**  \n",
        "  * **Category Distribution:** Create a bar chart to visualize the number of articles in each category. Check for class imbalance.  \n",
        "  * **Text Length Analysis:**  \n",
        "    * Calculate the length of each article (word count and character count).  \n",
        "    * Plot histograms or boxplots of article lengths for each category to see if there are any noticeable differences.  \n",
        "  * **Word Frequency Analysis:**  \n",
        "    * Identify the most common words in the entire corpus.  \n",
        "    * Create word clouds for each category to visualize the most frequent and important words.  \n",
        "    * Use bar charts to show the frequency of top N words per category after removing stopwords."
      ],
      "metadata": {
        "id": "ZnPfmSk0hXwG"
      },
      "id": "ZnPfmSk0hXwG"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lcGf-6mjhZM5"
      },
      "id": "lcGf-6mjhZM5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **3\\. Data Preprocessing**\n",
        "\n",
        "* **3.1. Text Cleaning:**  \n",
        "  * Convert all text to lowercase.  \n",
        "  * Remove punctuation and special characters.  \n",
        "  * Remove numbers (if they are not considered useful features).  \n",
        "  * Remove common English stopwords.  \n",
        "* **3.2. Text Normalization:**  \n",
        "  * **Lemmatization or Stemming:** Apply one of these techniques to reduce words to their root form. Explain the choice (lemmatization is generally preferred for better accuracy).  \n",
        "* **3.3. Feature Engineering (Text Representation):**  \n",
        "  * **TF-IDF (Term Frequency-Inverse Document Frequency):**  \n",
        "    * Explain the concept of TF-IDF.  \n",
        "    * Use TfidfVectorizer from scikit-learn to convert the preprocessed text into numerical vectors.  \n",
        "    * Discuss key parameters like max\\_features, ngram\\_range, and min\\_df/max\\_df.  \n",
        "* **3.4. Data Splitting:**  \n",
        "  * Split the data into training and validation sets using train\\_test\\_split. Ensure a stratified split if there is a class imbalance.\n",
        "\n",
        "### **4\\. Model Building**\n",
        "\n",
        "* **4.1. Baseline Model:**  \n",
        "  * Start with a simple, interpretable model like **Naive Bayes** (specifically MultinomialNB) or **Logistic Regression**.  \n",
        "  * Train the model on the TF-IDF vectors.  \n",
        "* **4.2. Advanced Models:**  \n",
        "  * Train a few more powerful models. Good candidates include:  \n",
        "    * **Support Vector Machines (SVM)**  \n",
        "    * **Random Forest**  \n",
        "    * **Gradient Boosting Machines (e.g., XGBoost, LightGBM)**  \n",
        "* **4.3. (Optional) Deep Learning Models:**  \n",
        "  * For a more advanced approach, consider a simple neural network:  \n",
        "    * **Word Embeddings (e.g., GloVe, Word2Vec) or an Embedding Layer.**  \n",
        "    * **Recurrent Neural Network (RNN) like LSTM or a Convolutional Neural Network (CNN) for text classification.**  \n",
        "    * This section would require libraries like TensorFlow/Keras or PyTorch.\n",
        "\n",
        "### **5\\. Model Evaluation**\n",
        "\n",
        "* **5.1. Performance Metrics:**  \n",
        "  * Define the evaluation metrics to be used. For a classification task, these include:  \n",
        "    * **Accuracy:** Overall correct predictions.  \n",
        "    * **Precision, Recall, F1-Score:** Per-class performance.  \n",
        "    * **Confusion Matrix:** To visualize where the models are making mistakes.  \n",
        "    * **Classification Report:** A summary of precision, recall, and F1-score for each class.  \n",
        "* **5.2. Model Comparison:**  \n",
        "  * Make predictions on the validation set for each trained model.  \n",
        "  * Generate a classification report and a confusion matrix for each model.  \n",
        "  * Create a summary table or bar chart to compare the key metrics (e.g., accuracy, F1-score) across all models.  \n",
        "  * Select the best-performing model based on the evaluation results.\n",
        "\n",
        "### **6\\. Hyperparameter Tuning (for the Best Model)**\n",
        "\n",
        "* **6.1. Tuning Strategy:**  \n",
        "  * Choose a hyperparameter tuning technique like **GridSearchCV** or **RandomizedSearchCV** for the best-performing model from the previous step.  \n",
        "  * Define the parameter grid to search over.  \n",
        "* **6.2. Final Model Training:**  \n",
        "  * Train the selected model with the best hyperparameters found during tuning on the **entire training dataset**.\n",
        "\n",
        "\n",
        "### **8\\. Comparison with supervised learning**\n",
        "\n",
        "\n",
        "\n",
        "## Reference\n",
        "* https://www.kaggle.com/competitions/learn-ai-bbc/overview\n",
        "\n",
        "---\n",
        "---\n",
        "\n",
        "### **7\\. Conclusion & Submission**\n",
        "\n",
        "* **7.1. Summary of Results:**  \n",
        "  * Summarize the project findings. State which model performed best and its final score on the validation set.  \n",
        "  * Discuss any interesting insights from the EDA or model performance.  \n",
        "* **7.2. (If applicable) Submission:**  \n",
        "  * Describe the process for generating the submission file if the competition requires predictions on a separate test set.  \n",
        "  * Load the test data, apply the same preprocessing steps, and use the final trained model to make predictions.  \n",
        "  * Format the predictions into the required submission file format.  \n",
        "* **7.3. Future Work:**  \n",
        "  * Suggest potential improvements, such as:  \n",
        "    * Trying more advanced deep learning architectures (e.g., Transformers like BERT).  \n",
        "    * Experimenting with different feature engineering techniques.  \n",
        "    * Using different word embeddings.  \n",
        "    * Ensemble methods."
      ],
      "metadata": {
        "id": "uEdAMy3lXSdb"
      },
      "id": "uEdAMy3lXSdb"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "78iWTqDQNpti"
      },
      "id": "78iWTqDQNpti"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb6c5005",
      "metadata": {
        "id": "eb6c5005"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}